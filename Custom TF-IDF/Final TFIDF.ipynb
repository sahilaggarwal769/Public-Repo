{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "# Compare your results with the above sklearn tfidf vectorizer\n",
    "# You are not supposed to use any other library apart from the ones given below\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):    \n",
    "    unique_words = set() \n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: \n",
    "            for word in row.split(\" \"): \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        \n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(fit(corpus).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=fit(corpus)\n",
    "#unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom IDF calculator method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(unique_words):\n",
    "    idf=[]\n",
    "    for word in unique_words:\n",
    "        cnt=0\n",
    "        for row in corpus:\n",
    "            if word in row:\n",
    "                cnt+=1\n",
    "        idf_value=1+math.log((1+len(corpus))/(1+cnt))\n",
    "        idf.append(idf_value)\n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing IDF value of sklearn and custom method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "idf=calculate_idf(list(fit(corpus).keys()))\n",
    "print(idf)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,unique_words,idf):\n",
    "    rows=[]\n",
    "    columns=[]\n",
    "    values=[]\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(tqdm(dataset)): # for each document in the dataset\n",
    "            # it will return a dict type object where key is the word and values is its frequency, {word:frequency}\n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            # for every unique word in the document\n",
    "            for word, freq in word_freq.items():  # for each unique word in the review.                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                # we will check if its there in the vocabulary that we build in fit() function\n",
    "                # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                col_index = unique_words.get(word, -1) # retreving the dimension number of a word\n",
    "                \n",
    "                # if the word exists\n",
    "                if col_index !=-1:\n",
    "                    # we are storing the index of the document\n",
    "                    rows.append(idx)\n",
    "                    # we are storing the dimensions of the word\n",
    "                    columns.append(col_index)\n",
    "                    # we are storing the frequency of the word\n",
    "                    tf=freq/len(row)\n",
    "                    #print(\"tf=\",tf)\n",
    "                    #print(\"col index=\",col_index)\n",
    "                    idf_val=idf[col_index]\n",
    "                    #print(\"idf=\",idf_val)\n",
    "                    tf_idf=tf*idf_val\n",
    "                    values.append(tf_idf)\n",
    "        csr=csr_matrix((values, (rows,columns)), shape=(len(dataset),len(unique_words)))\n",
    "        normalize_vector=normalize(csr,norm='l2')\n",
    "        \n",
    "        #print(skl_output[0])\n",
    "        #print(\"csr=\",csr)\n",
    "        #print(\"norm=\",normalize_vector)\n",
    "        return normalize_vector\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the result of sklearn's tranform method and custorm tranform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(skl_output[0].toarray())\n",
    "print(transform(corpus,unique_words,idf)[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom fit method for Task2 containing all unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):    \n",
    "    unique_words = set() \n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: \n",
    "            for word in row.split(\" \"): \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        \n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=fit(corpus)\n",
    "#print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom IDF calculator method for task-2 containin top 50 features (idf values) and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(unique_words):\n",
    "    idf=[]\n",
    "    uni_word=[]\n",
    "    for word in unique_words:\n",
    "        cnt=0\n",
    "        for row in corpus:\n",
    "            if word in row:\n",
    "                cnt+=1\n",
    "        idf_value=1+math.log((1+len(corpus))/(1+cnt))\n",
    "        uni_word.append(word)\n",
    "        idf.append(idf_value)\n",
    "        #print(\"idf=\",len(idf))\n",
    "        #print(\"unique word=\",len(uni_word))\n",
    "    #print(len(idf))\n",
    "    #print(uni_word)\n",
    "    #print(\"len uni=\",len(uni_word))\n",
    "    #print(\"len idf=\",len(idf))\n",
    "    cnt=0\n",
    "    #print(max(idf))\n",
    "    \"\"\"for i in idf:\n",
    "        if i==max(idf):\n",
    "            cnt+=1\n",
    "    #print(cnt)\"\"\"\n",
    "    for i in range(len(idf)):\n",
    "        for j in range(len(idf)):\n",
    "            if(idf[i]>idf[j]):\n",
    "                idf[j],idf[i]=idf[i],idf[j]\n",
    "            \n",
    "                uni_word[j],uni_word[i]=uni_word[i],uni_word[j]\n",
    "        #print(idf[:50])\n",
    "        #print(uni_word[:50])\n",
    "    top_50_idf=idf[:50]\n",
    "        #print(\"idf=\",len(top_50_idf))\n",
    "    vocab={j:i for i,j in enumerate(uni_word[:50])}\n",
    "    #print(\"vocab=\",len(vocab))\n",
    "        #for i in range(len(vocab)):\n",
    "            #print(\"word=\",list(vocab.keys())[i],\",idf=\",top_50_idf[i])\n",
    "    return (top_50_idf,vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(unique_words))\n",
    "#print(len(calculate_idf(list(fit(corpus).keys()))))\n",
    "top_50_idf,vocab=calculate_idf(list(fit(corpus).keys()))\n",
    "#print(top_50_idf)\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transform method for Task-2 where tf*idf value is calculated for whole corpus based on top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,unique_words,idf):\n",
    "    rows=[]\n",
    "    columns=[]\n",
    "    values=[]\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(tqdm(dataset)): # for each document in the dataset\n",
    "            # it will return a dict type object where key is the word and values is its frequency, {word:frequency}\n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            # for every unique word in the document\n",
    "            for word, freq in word_freq.items():  # for each unique word in the review.                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                # we will check if its there in the vocabulary that we build in fit() function\n",
    "                # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                col_index = unique_words.get(word, -1) # retreving the dimension number of a word\n",
    "                \n",
    "                # if the word exists\n",
    "                if col_index !=-1:\n",
    "                    # we are storing the index of the document\n",
    "                    rows.append(idx)\n",
    "                    # we are storing the dimensions of the word\n",
    "                    columns.append(col_index)\n",
    "                    # we are storing the frequency of the word\n",
    "                    tf=freq/len(row)\n",
    "                    #print(\"tf=\",tf)\n",
    "                    #print(\"col index=\",col_index)\n",
    "                    idf_val=idf[col_index]\n",
    "                    #print(\"idf=\",idf_val)\n",
    "                    tf_idf=tf*idf_val\n",
    "                    values.append(tf_idf)\n",
    "        csr=csr_matrix((values, (rows,columns)), shape=(len(dataset),len(unique_words)))\n",
    "        normalize_vector=normalize(csr,norm='l2')\n",
    "        return normalize_vector\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of custom normalized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 746/746 [00:00<00:00, 65731.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(transform(corpus,vocab,top_50_idf).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation->\n",
    "As there are 746 documents in the corpus and the number of features are 50 Shape=(746,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of a single document in your collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 746/746 [00:00<00:00, 83385.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(transform(corpus,vocab,top_50_idf)[0].toarray())\n",
    "#print(skl_output[0].toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
